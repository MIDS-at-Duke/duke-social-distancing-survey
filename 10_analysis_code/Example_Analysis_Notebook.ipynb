{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Analysis Code\n",
    "\n",
    "What follows are some basic analyses of the Duke Forge/SSRI COVID19 Digital Lab Social Distancing Survey Weeks 1 & 2.\n",
    "\n",
    "Because this is a *weighted* survey, note that simple summary statistics will not accurately reflect population statistics. This notebook includes code that can be easily modified for calculating basically statistics using these weights. \n",
    "\n",
    "Also note that many variables have top-codes of `9`. This is because surveys were conducted with automated calling, so respondents had to punch in values on their phones. Thus be careful in interpreting top-values. \n",
    "\n",
    "The code the follows is written in Python and requires both the `pandas` and `statsmodels` packages, both of which can easily be installed using `conda` or `pip`. \n",
    "\n",
    "R users who wish to do their own analyses will likely find the `Survey` package to be of use. \n",
    "\n",
    "Note this data includes ALL responses, including partials. As a result, the composition of your sample may change if you start using demographic controls that occur later in the survey (and thus are less likely to be included for a given respondent). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating Number of Children Per Household\n",
    "\n",
    "Before analysis, we being by replicating one of the summary statistics provided by the firm that conducted this survey. Top-line summary statistics from the survey firm can be found in `40_reports/Survey_Summaries`. \n",
    "\n",
    "In particular, we'll replicate the share of households with No Children, One Child, and Two Children. The survey firm has reported that, after adjusting for weights, the proportion of households of each type are: \n",
    "\n",
    "- No Children: 60\\% (Week 1), 65\\% (Week 2)\n",
    "- One Child: 15\\% (Week 1), 13\\% (Week 2)\n",
    "- Two Children: 17\\% (Week 1), 15\\% (Week 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uniqueID', 'Date', 'Voter File Match',\n",
       "       'Registered Voter (of Voter File Matches)', 'weight',\n",
       "       'Q1. Health Quality', 'age', 'DEMOGRAPHICS - GENDER',\n",
       "       'Q4. Number of People in HH', 'Q5. Children in HH',\n",
       "       'Q6. Non-HH Face to Face Count', 'Q7. Six Feet Away? (If Q6 > 0)',\n",
       "       'Q8. HH Member Going to Work',\n",
       "       'Q9. Children Interacting with Other Children ',\n",
       "       'Q10. Times in Group > 20 in Last Week', 'Family', 'Friends',\n",
       "       'Co-workers', 'Clients, patients, or patrons',\n",
       "       'Any other type of person not already mentioned',\n",
       "       'Q12. Handwashing Count',\n",
       "       'Q13. Currently Practicing Social Distancing?',\n",
       "       'Q14. Currently Experiencing Symptoms?',\n",
       "       'Q15. Likelihood of getting Coronavirus',\n",
       "       'Q16. NC Response to Coronavirus', 'Q17. Changes to Routine ',\n",
       "       'Q18. College Degree', 'Q19. Latino', 'Q20. Race',\n",
       "       'Q21. Panel Willingness', 'Q19-20. Race + Ethnicity', 'Survey Mode',\n",
       "       'DEMOGRAPHICS - RACE ON FILE', 'MEDIA MARKET', 'PARTY', 'TURNOUT',\n",
       "       'DEMOGRAPHICS - PARTY ON FILE', 'tax_score', 'CHOICE',\n",
       "       'CHURCH ATTENDANCE', 'CLIMATE CHANGE PRIORITY', 'COLLEGE',\n",
       "       'GUN CONTROL', 'GUN OWNER', 'PROGRESSIVE GUN OWNER', 'tv_cable',\n",
       "       'tv_satellite', 'tv_broadcast', 'cellphone_score', 'commercial_party',\n",
       "       'MARIJUANA', 'low_tv', 'trump_approve_score', 'retired_score',\n",
       "       'county_name', 'Completed Survey', 'week', 'Panel Respondent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned, and labeled version \n",
    "# of the survey. \n",
    "# The code that generates this cleaned data\n",
    "# can be found in `10_import_and_format_week1.py`. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "svy = pd.read_csv('../20_analysis_datasets/'\n",
    "                  'merged_surveys.csv')\n",
    "svy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a few convenience vars: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "svy['week1'] = svy['week'] == 'week1'\n",
    "svy['week2'] = svy['week'] == 'week2'\n",
    "svy['full_sample'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all variables up to Q21 are questions asked in this survey. Subsequent variables come largely from Clarity Campaigns, which has done its best to match survey respondents with an internal database built off other sources. Information on these variables can be found in `40_reports`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3513"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can replicate results from Clarity. For this we will rely on the `DescrStatsW` function from `statsmodels`. [Documentation can be found here](https://www.statsmodels.org/stable/generated/statsmodels.stats.weightstats.DescrStatsW.html), but the basic idea is that it takes a vector of data, a vector of weights, and then returns various weighted statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of households with None kids in week1 is 60.3%\n",
      "Share of households with One kids in week1 is 14.8%\n",
      "Share of households with Two kids in week1 is 17.1%\n",
      "Share of households with None kids in week2 is 64.9%\n",
      "Share of households with One kids in week2 is 12.8%\n",
      "Share of households with Two kids in week2 is 14.5%\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "\n",
    "for week in ['week1', 'week2']:\n",
    "    \n",
    "    oneweek = svy[svy['week'] == week].copy()\n",
    "    for num in ['None', 'One', 'Two']:\n",
    "        oneweek[num] = oneweek['Q5. Children in HH'] == num\n",
    "        temp = oneweek[pd.notnull(oneweek['Q5. Children in HH'])]    \n",
    "        w = DescrStatsW(temp[num], temp['weight'])\n",
    "        print(f\"Share of households with {num} kids in {week} is {w.mean:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, calculating overall averages is only kinda interesting. Generally, we want to know statistics for sub-populations. Normally, we'd just do this using the `groupby` operator, but things are a little more complicated with weighting. \n",
    "\n",
    "To begin, let's again calculate the proportion of households with different numbers of children using groupby for the trivial case where everyone is in the same group. Once we know that works, we can start looking at more interesting sub-populations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share households with None kids in Week 1: 60.3%\n",
      "Share households with One kids in Week 1: 14.8%\n",
      "Share households with Two kids in Week 1: 17.1%\n"
     ]
    }
   ],
   "source": [
    "def get_group_mean(data, question):\n",
    "    temp = data[[question, 'weight']]\n",
    "    temp = temp[pd.notnull(temp[question])]\n",
    "    wsvy = DescrStatsW(temp[question], temp['weight'])\n",
    "    return wsvy.mean\n",
    "\n",
    "for num in ['None', 'One', 'Two']:\n",
    "    week1 = svy[svy['week'] == 'week1'].copy()\n",
    "    week1[num] = (week1['Q5. Children in HH'] == num)\n",
    "    week1 = week1[pd.notnull(week1['Q5. Children in HH'])]    \n",
    "\n",
    "    week1['dummy'] = 1\n",
    "\n",
    "    raw = week1[num].mean()\n",
    "\n",
    "    w = week1.groupby('dummy').apply(lambda x: get_group_mean(x, num))\n",
    "    w = w.iloc[0]\n",
    "    \n",
    "    print(f'Share households with {num} kids in Week 1: {w:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's break down number of children by race. To do this, we'll first re-code race since most categories are too small to be statistically valuable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "White                 2525\n",
       "Black                  751\n",
       "Another race           104\n",
       "Hispanic or Latino      92\n",
       "Asian                   41\n",
       "Name: Q19-20. Race + Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values before grouping\n",
    "race = 'Q19-20. Race + Ethnicity'\n",
    "svy[race].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "White    2525\n",
       "Black     751\n",
       "Other     237\n",
       "Name: Q19-20. Race + Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svy[race] = svy[race].replace({'Asian': 'Other',\n",
    "                               'Hispanic or Latino': 'Other',\n",
    "                               'Another race': 'Other'})\n",
    "svy[race].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at number of children by racial group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q19-20. Race + Ethnicity\n",
       "Black    0.571685\n",
       "Other    0.539243\n",
       "White    0.597149\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svy['None'] = svy['Q5. Children in HH'] == 'None'\n",
    "temp = svy[pd.notnull(svy['Q5. Children in HH'])]\n",
    "svy.groupby(race).apply(lambda x: get_group_mean(x, 'None'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing COVID-Related Outcomes\n",
    "\n",
    "Now that we've gotten the basics of working with weighted survey data out of the way (and by comparing our calculated values to known outcomes from the survey firm, we know we've done it correctly), let's start looking at some COVID-related variables!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Groups\n",
    "\n",
    "A key question in our analysis is whether people have been in large groups in the last week. Note that because this survey was conducted on the 29th, 30th, and 31st, most North Carolineans were not yet under a \"shelter-in-place\" order during the week preceding this survey, so we wouldn't expect people's answers to \"How many times have you been in a group of > 20 people in the last week\" to be all zeros!\n",
    "\n",
    "Note that `9` is a top-code here, so values of `9` mean \"9 or greater\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In week1, the number of (reported) times people had been in groups > 20 was:\n",
      "Q10. Times in Group > 20 in Last Week\n",
      "0.0    0.787501\n",
      "1.0    0.089631\n",
      "2.0    0.040359\n",
      "3.0    0.019480\n",
      "4.0    0.004135\n",
      "5.0    0.013134\n",
      "6.0    0.004822\n",
      "7.0    0.001940\n",
      "8.0    0.000546\n",
      "9.0    0.038451\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "In week2, the number of (reported) times people had been in groups > 20 was:\n",
      "Q10. Times in Group > 20 in Last Week\n",
      "0.0    0.823140\n",
      "1.0    0.088225\n",
      "2.0    0.034875\n",
      "3.0    0.011741\n",
      "4.0    0.002627\n",
      "5.0    0.005282\n",
      "6.0    0.009145\n",
      "7.0    0.000508\n",
      "8.0    0.000996\n",
      "9.0    0.023459\n",
      "dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get weighted proportions in each category\n",
    "big_group = 'Q10. Times in Group > 20 in Last Week'\n",
    "\n",
    "for week in ['week1', 'week2']:\n",
    "    def get_group_sumweights(data, question):\n",
    "        temp = data[[question, 'weight']]\n",
    "        temp = temp[pd.notnull(temp[question])]\n",
    "        wsvy = DescrStatsW(temp[question], temp['weight'])\n",
    "        return wsvy.sum_weights\n",
    "\n",
    "    temp = svy[svy['week'] == week]\n",
    "    sums = temp.groupby(big_group).apply(lambda x: get_group_sumweights(x, big_group))\n",
    "    proportions = sums / sums.sum()\n",
    "    print(f'In {week}, the number of (reported) times people had been in groups > 20 was:')\n",
    "    print(proportions)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly the *vast* majority of people haven't been in big groups (or won't admit to it). So let's just look at the share of people who've EVER been in a big group in the last year, and see how it breaks down by sub-population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "svy['any_group']= (svy[big_group] > 0) & pd.notnull(svy[big_group])\n",
    "svy.loc[pd.isnull(svy[big_group]), 'any_group'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share ever a large group in week week1\n",
      "21.2%\n",
      "\n",
      "\n",
      "share ever a large group in week week2\n",
      "17.7%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for week in ['week1', 'week2']:\n",
    "    print(f'share ever a large group in week {week}')\n",
    "    print(f\"{svy[svy[week]].groupby('full_sample').apply(lambda x: get_group_mean(x, 'any_group')).iloc[0]:.1%}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share ever a large group in week week1\n",
      "Q19-20. Race + Ethnicity\n",
      "Black    0.240851\n",
      "Other    0.264728\n",
      "White    0.196275\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "share ever a large group in week week2\n",
      "Q19-20. Race + Ethnicity\n",
      "Black    0.274025\n",
      "Other    0.181535\n",
      "White    0.151264\n",
      "dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And by race\n",
    "for week in ['week1', 'week2']:\n",
    "    print(f'share ever a large group in week {week}')\n",
    "    print(f\"{svy[svy.week == week].groupby(race).apply(lambda x: get_group_mean(x, 'any_group'))}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this suggests a significant decline in groups OTHER than `Black`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q19-20. Race + Ethnicity\n",
       "Black    0.259548\n",
       "Other    0.223341\n",
       "White    0.171468\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race = 'Q19-20. Race + Ethnicity'\n",
    "avg_num = svy.groupby(race).apply(lambda x: get_group_mean(x, 'any_group'))\n",
    "avg_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some small differences. Clarity provides a \"likelihood people attend church\" we can check to see if that's driving things (Black North Carolineans have slightly higher likelihood of attending church, says Clarity data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q19-20. Race + Ethnicity\n",
       "Black    3.428091\n",
       "Other    2.616084\n",
       "White    3.017049\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svy.groupby(race).apply(\n",
    "    lambda x: get_group_mean(x, 'CHURCH ATTENDANCE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look for variation in likelihood by education -- appears those without college degree more likely to be in groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q18. College Degree\n",
       "4         [0.0]\n",
       "No     0.255593\n",
       "Yes    0.154899\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "educ = 'Q18. College Degree'\n",
    "avg_num = svy.groupby(educ).apply(\n",
    "    lambda x: get_group_mean(x, 'any_group'))\n",
    "avg_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distancing in last 24 hours\n",
    "\n",
    "The survey also asks about number of people with whom one has had face-to-face interactions in the last 24 hours, then in how many of those interactions was the respondent able to maintain social distance. The difference is num of people they weren't able to keep distance with. \n",
    "\n",
    "However, note top-codes make interpreting this a little tricky..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svy['close_interacts'] = (svy['Q6. Non-HH Face to Face Count'] - \n",
    "                          svy['Q7. Six Feet Away? (If Q6 > 0)'])\n",
    "svy.loc[svy['Q6. Non-HH Face to Face Count'] == 0, 'close_interacts'] = 0\n",
    "svy.loc[svy['Q6. Non-HH Face to Face Count'] == 9, 'close_interacts'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0    1971\n",
       " 1.0     289\n",
       " 2.0     157\n",
       " 3.0      78\n",
       " 4.0      35\n",
       "-1.0      32\n",
       " 5.0      20\n",
       "-3.0      13\n",
       " 6.0      11\n",
       "-7.0       9\n",
       "-8.0       9\n",
       "-2.0       8\n",
       "-6.0       8\n",
       "-4.0       7\n",
       " 7.0       7\n",
       "-5.0       7\n",
       " 8.0       3\n",
       "Name: close_interacts, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svy.close_interacts.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, negatives are clearly junk..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svy.loc[svy['close_interacts'] < 0, 'close_interacts'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "close_interacts\n",
       "0.0    0.787248\n",
       "1.0    0.093663\n",
       "2.0    0.064381\n",
       "3.0    0.025122\n",
       "4.0    0.012273\n",
       "5.0    0.006736\n",
       "6.0    0.003029\n",
       "7.0    0.004234\n",
       "8.0    0.003314\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums = svy.groupby('close_interacts').apply(\n",
    "           lambda x: get_group_sumweights(x, 'close_interacts'))\n",
    "proportions = sums / sums.sum()\n",
    "proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 11% said they had at least one interaction without distancing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
